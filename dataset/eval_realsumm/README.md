# REALSumm Dataset

## Description
**REALSumm** is a meta-evaluation dataset for text summarization which measures pyramid recall of each system-generated summary. It contains human annotations for top-scoring system generations on 100 documents from the popular CNNDM dataset.


## Meta Data
* Github Repo: [neulab/REALSumm](https://github.com/neulab/REALSumm)
* Paper: [Re-evaluating Evaluation in Text Summarization](https://aclanthology.org/2020.emnlp-main.751/) 
* Aspect: pyramid recall
* Language: English


## Data Structure
### Example
We collate the human judgements inside `data.json`. A data sample consists of a key and its value is shown in the following format.
```
{
  "src": "This is the source text.", 
  "refs": ["This is the reference summary."], 
  "hypos": 
  {
      "presumm_out_trans_abs.txt": 
      {
          "hypo": "This is the summary generated by system presumm_out_trans_abs.", 
          "scores": {"litepyramid_recall": "0.3"}
      }, 
      "two_stage_rl_out.txt": 
      {
          "hypo": "This is the summary generated by system two_stage_rl_out.", 
          "scores": {"litepyramid_recall": "0.1"}
      }, 
      ...
  }
}
```

### Format
Each data sample contains the following fields:
* `src`: The tokenized, normal-cased source text.
* `refs`: The tokenized, normal-cased reference summaries.
* `hypos`: Collected system summaries together with their human judgement scores.
    * `hypo`: The tokenized, normal-cased system generated summary.
    * `scores`: (We use string scores to save space) For `data.json`, there are only human judgement scores stored as key-value dictionary here. For system output files, the automatic metric scores will also be displayed here (e.g. `{"auto-metric1": "0.2"}`).


## Reference
```
@inproceedings{bhandari-etal-2020-evaluating,
    title = "Re-evaluating Evaluation in Text Summarization",
    author = "Bhandari, Manik  and
      Gour, Pranav Narayan  and
      Ashfaq, Atabak  and
      Liu, Pengfei  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.751",
    doi = "10.18653/v1/2020.emnlp-main.751",
    pages = "9347--9359",
    abstract = "Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not {--} for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).",
}
```
