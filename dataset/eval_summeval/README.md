# SummEval Dataset

## Description
**SummEval** is a collection of human
judgments of model-generated summaries on the subset (100 samples) of CNNDM annotated by both expert judges and
crowd-source workers. Each system generated summary is gauged through the lens of *coherence*, *consistency*, *fluency* and *relevance*.


## Meta Data
* Github Repo: [Yale-LILY/SummEval](https://github.com/Yale-LILY/SummEval)
* Paper: [SummEval: Re-evaluating Summarization Evaluation](https://arxiv.org/abs/2007.12626)
* Aspect: coherence, consistency, fluency, relevance
* Language: English


## Data Structure
### Example
We collate the human judgements inside `data.json`. A data sample consists of a key and its value is shown in the following format.

```
{
    'src': "This is the source text",
    'refs': [
                    "This is the reference summary1", 
                    "This is the reference summary2",
                    ...
                ],
    'hypos': {
        'M11': {
            'hypo': "This is the summary generated by system M11.",
            'scores': {
                        'coherence': '1.3333333333333333',
                        'consistency': '1.0',
                        'fluency': '3.0',
                        'relevance': '1.6666666666666667'
                       }
                },
        'M13': {
            'hypo': "This is the summary generated by system M13.",
            'scores': {
                        'coherence': '2.3333333333333335',
                        'consistency': '5.0',
                        'fluency': '5.0',
                        'relevance': '2.6666666666666665'
                       }
                },
        ...
    },
}
```

### Format
Each data sample contains the following fields:
* `src`: The tokenized, normal-cased source text.
* `refs`: Multiple  tokenized, normal-cased reference summaries
* `hypos`: Collected system summaries together with their human judgement scores.
    * `hypo`: The tokenized, normal-cased system generated summaries.
    * `scores`: (We use string scores to save space) For `data.json`, there are only human judgement scores stored as key-value dictionary here. For system output files, the automatic metric scores will also be displayed here (e.g. `{"auto-metric1": "0.2"}`).


## Reference
```
@misc{fabbri2021summeval,
      title={SummEval: Re-evaluating Summarization Evaluation}, 
      author={Alexander R. Fabbri and Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher and Dragomir Radev},
      year={2021},
      eprint={2007.12626},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```